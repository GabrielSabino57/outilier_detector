# -*- coding: utf-8 -*-
"""outilier_detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/119xnEhWdR-bhoHYMYR-ZtX-kEkY_IQW0

# ==========================================================
# Script para detec√ß√£o de outliers com Random Forest, Extratree, GradienteBuster e MPL (Perceptron Multicamadas - RNA)
# Vers√£o com multiplas nuvens de pontos: x y z label
# Vers√£o utilizando estat√≠sticas locais e dentro de um cilindro
# Vers√£o com valida√ß√£o do modelo ao final do treinamento
# ===========================================================
"""

# ---------------------------------------------------------
!pip install scikit-learn pandas numpy matplotlib tqdm --quiet

import os
import io
import numpy as np
import pandas as pd
from google.colab import files
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

from sklearn.neighbors import NearestNeighbors
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

# -----------------------------
# Configura√ß√µes
# -----------------------------
K_NEIGHBORS = 20            # n√∫mero de vizinhos para features locais (em XY)
R_FACTOR = 3.0             # R = R_FACTOR * median(nn_distances_in_XY)
MIN_POINTS_FOR_CYL = 5     # m√≠nimo de vizinhos no cilindro para calcular estat√≠sticas
TEST_SIZE = 0.2
RANDOM_STATE = 42
PLOT_SHOW = True           # mostrar plots inline
SAVE_PLOTS = True          # salvar arquivos de figura
OUT_FOLDER = "results_pipeline"
os.makedirs(OUT_FOLDER, exist_ok=True)

# -----------------------------
# Leitura flex√≠vel de um arquivo:
# aceita header (x y z label) ou sem header (3/4 colunas)
# retorna df padronizado com colunas: x, y, z, label
# -----------------------------
def read_pointcloud_flexible(path_or_bytes, name_for_errors="file"):
    """
    path_or_bytes: caminho ou bytes (quando usando files.upload)
    """
    # tenta ler com header
    try:
        if isinstance(path_or_bytes, (bytes, bytearray)):
            df_try = pd.read_csv(io.BytesIO(path_or_bytes), sep=r"\s+", engine="python")
        else:
            df_try = pd.read_csv(path_or_bytes, sep=r"\s+", engine="python")
        cols_low = [c.strip().lower() for c in df_try.columns]
        if set(["x","y","z","label"]).issubset(cols_low):
            # padroniza ordem e nomes
            mapping = {orig: low for orig, low in zip(df_try.columns, cols_low)}
            df_try = df_try.rename(columns=mapping)
            df = df_try[["x","y","z","label"]].copy()
            df[['x','y','z']] = df[['x','y','z']].astype(float)
            df['label'] = df['label'].astype(int)
            return df
        # caso header presente mas nomes diferente, tenta for√ßar por posi√ß√£o:
        if df_try.shape[1] >= 4:
            df_try = df_try.iloc[:, :4]
            df_try.columns = ['x','y','z','label']
            df_try[['x','y','z']] = df_try[['x','y','z']].astype(float)
            df_try['label'] = df_try['label'].astype(int)
            return df_try
        # se tem exatamente 3 colunas e header era texto (ex: 'x y z'), √© OK:
        if df_try.shape[1] == 3:
            df_try.columns = ['x','y','z']
            df_try['label'] = 0
            df_try[['x','y','z']] = df_try[['x','y','z']].astype(float)
            return df_try
    except Exception as e:
        # tenta leitura sem header (for√ßando nomes)
        pass

    # fallback: ler sem header e atribuir colunas
    try:
        if isinstance(path_or_bytes, (bytes, bytearray)):
            df = pd.read_csv(io.BytesIO(path_or_bytes), sep=r"\s+", header=None, engine="python")
        else:
            df = pd.read_csv(path_or_bytes, sep=r"\s+", header=None, engine="python")
        if df.shape[1] >= 4:
            df = df.iloc[:, :4]
            df.columns = ['x','y','z','label']
        elif df.shape[1] == 3:
            df.columns = ['x','y','z']
            df['label'] = 0
        else:
            raise ValueError(f"{name_for_errors}: arquivo com n√∫mero inesperado de colunas ({df.shape[1]})")
        df[['x','y','z']] = df[['x','y','z']].astype(float)
        df['label'] = df['label'].fillna(0).astype(int)
        return df
    except Exception as e:
        raise ValueError(f"Falha ao ler {name_for_errors}: {e}")

# -----------------------------
# Feature extraction por nuvem
# - vizinhan√ßa selecionada em XY (NearestNeighbors usando XY)
# - covari√¢ncia usando pontos 3D para curvatura
# - raio R adaptativo = R_FACTOR * median(nn_distances_xy)
# Retorna df com colunas de features (mant√©m x,y,z,label originais)
# -----------------------------
def extract_features_for_cloud(df, k_neighbors=K_NEIGHBORS, r_factor=R_FACTOR):
    pts = df[['x','y','z']].values
    pts_xy = df[['x','y']].values

    N = len(df)
    # --- nearest neighbors in XY to compute typical spacing ---
    nbrs_xy = NearestNeighbors(n_neighbors=min(k_neighbors+1, N), algorithm='auto').fit(pts_xy)
    dists_xy_all, idxs_xy_all = nbrs_xy.kneighbors(pts_xy)
    # remove self (first column distance zero)
    dists_nn = dists_xy_all[:, 1] if dists_xy_all.shape[1] > 1 else dists_xy_all[:, 0]
    median_nn = np.median(dists_nn)
    R = max(1e-6, r_factor * median_nn)  # raio adaptativo (evita zero)

    # para features locais usamos k neighbors (em XY), mas para cov usamos os mesmos vizinhos em 3D
    k_use = min(k_neighbors+1, N)
    nbrs_xy_k = NearestNeighbors(n_neighbors=k_use, algorithm='auto').fit(pts_xy)
    dists_k, idxs_k = nbrs_xy_k.kneighbors(pts_xy)  # includes self as first neighbor

    # inicializa listas
    densidade = np.zeros(N)
    dist_media = np.zeros(N)
    curvatura = np.zeros(N)
    linearidade = np.zeros(N)
    planaridade = np.zeros(N)
    rugosidade = np.zeros(N)
    declividade = np.zeros(N)

    for i in range(N):
        idxs = idxs_k[i]
        # remove self index (geralmente primeiro)
        idxs_n = idxs[idxs != i]
        viz = pts[idxs_n] if len(idxs_n)>0 else pts[idxs]
        densidade[i] = len(viz)
        dist_media[i] = np.mean(np.linalg.norm(viz - pts[i], axis=1)) if len(viz)>0 else 0.0

        # covariancia em 3D (inclui Z)
        if len(viz) >= 3:
            cov = np.cov(viz.T)
            eigvals, _ = np.linalg.eigh(cov)
            eigvals = np.sort(np.maximum(eigvals, 0))  # evita negativos numericos
            s = eigvals.sum()
            if s > 0:
                curvatura[i] = eigvals[0] / s
                linearidade[i] = (eigvals[2] - eigvals[1]) / eigvals[2] if eigvals[2] > 0 else 0
                planaridade[i] = (eigvals[1] - eigvals[0]) / eigvals[2] if eigvals[2] > 0 else 0
            else:
                curvatura[i] = linearidade[i] = planaridade[i] = 0.0
        else:
            curvatura[i] = linearidade[i] = planaridade[i] = 0.0

        # rugosidade: desv padrao em Z dos vizinhos (3D)
        rugosidade[i] = np.std(viz[:,2]) if len(viz)>0 else 0.0

        # declividade m√©dia: inclina√ß√£o relativa aos vizinhos
        delta_z = viz[:,2] - pts[i,2] if len(viz)>0 else np.array([0.0])
        delta_xy = np.linalg.norm(viz[:,:2] - pts[i,:2], axis=1) if len(viz)>0 else np.array([0.0])
        slope_deg = np.degrees(np.mean(np.arctan2(np.abs(delta_z), delta_xy + 1e-8)))
        declividade[i] = slope_deg

    # --- Estat√≠sticas dentro do cilindro de raio R (em XY) ---
    dens_cil = np.zeros(N)
    dist_cil = np.zeros(N)
    curv_cil = np.zeros(N)
    for i in range(N):
        # calcula distancias horizontais em XY
        dxy = np.linalg.norm(pts_xy - pts_xy[i], axis=1)
        mask = dxy < R
        pts_mask = pts[mask]
        if pts_mask.shape[0] < MIN_POINTS_FOR_CYL:
            dens_cil[i] = 0
            dist_cil[i] = 0
            curv_cil[i] = 0
            continue
        dens_cil[i] = pts_mask.shape[0]
        dist_cil[i] = np.mean(np.linalg.norm(pts_mask - pts[i], axis=1))
        cov = np.cov(pts_mask.T)
        eigvals, _ = np.linalg.eigh(cov)
        eigvals = np.sort(np.maximum(eigvals, 0))
        s = eigvals.sum()
        curv_cil[i] = eigvals[0] / s if s>0 else 0.0

    # monta dataframe de features
    features_df = pd.DataFrame({
        'x': df['x'].values,
        'y': df['y'].values,
        'z': df['z'].values,
        'label': df['label'].values,
        'densidade': densidade,
        'dist_media': dist_media,
        'curvatura': curvatura,
        'linearidade': linearidade,
        'planaridade': planaridade,
        'rugosidade': rugosidade,
        'declividade': declividade,
        'densidade_cilindro': dens_cil,
        'dist_media_cilindro': dist_cil,
        'curvatura_cilindro': curv_cil,
        'R_used': R  # mesmo valor para toda a nuvem
    })
    return features_df

# -----------------------------
# Upload m√∫ltiplo ou lista manual
# -----------------------------
print("Fa√ßa upload de 1+ arquivos (.txt) contendo x y z label (com ou sem cabe√ßalho).")
uploaded = files.upload()
file_names = list(uploaded.keys())
# --- Se preferir, voc√™ pode substituir 'file_names' por uma lista local, ex:
# file_names = ["plano.txt", "sela.txt"]

# -----------------------------
# Extrair features por arquivo, e armazenar
# -----------------------------
clouds_feats = {}   # nome -> df_features
for fname in file_names:
    print(f"\nLendo {fname} ...")
    raw_bytes = uploaded[fname]
    df_raw = read_pointcloud_flexible(raw_bytes, name_for_errors=fname)
    feats = extract_features_for_cloud(df_raw)
    clouds_feats[fname] = feats
    print(f"-> extra√≠do: {len(feats)} pontos, R_used = {feats['R_used'].iloc[0]:.4g}")

# -----------------------------
# Montar dataset de treino: concatenar todas as nuvens
# -----------------------------
all_feats = pd.concat(list(clouds_feats.values()), ignore_index=True)
feature_cols = [
    'densidade','dist_media','curvatura','linearidade','planaridade',
    'rugosidade','declividade','densidade_cilindro','dist_media_cilindro','curvatura_cilindro'
]
X_all = all_feats[feature_cols].fillna(0).values
y_all = all_feats['label'].values

# normaliza
scaler = StandardScaler()
X_all_scaled = scaler.fit_transform(X_all)

# stratified split (se houver ambas as classes)
stratify_arg = y_all if len(np.unique(y_all))>1 else None
X_train, X_test, y_train, y_test = train_test_split(
    X_all_scaled, y_all, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=stratify_arg
)

# -----------------------------
# Treinar modelos (com todos os dados concatenados)
# -----------------------------
models = {
    "RandomForest": RandomForestClassifier(n_estimators=200, random_state=RANDOM_STATE),
    "ExtraTrees": ExtraTreesClassifier(n_estimators=200, random_state=RANDOM_STATE),
    "GradientBoosting": GradientBoostingClassifier(n_estimators=200, random_state=RANDOM_STATE),
    "MLP": MLPClassifier(hidden_layer_sizes=(64,32), max_iter=1000, random_state=RANDOM_STATE)
}

trained_models = {}
for name, model in models.items():
    print(f"\nTreinando {name} ...")
    model.fit(X_train, y_train)
    trained_models[name] = model
    # opcional: relat√≥rio r√°pido no conjunto de teste global
    y_pred_glob = model.predict(X_test)
    print(f"Resumo global (teste) - {name}:")
    print(classification_report(y_test, y_pred_glob))

# -----------------------------
# Avalia√ß√£o separada por nuvem (usar o scaler treinado)
# -----------------------------
summary = []
for fname, feats in clouds_feats.items():
    print(f"\n\n===== Avaliando nuvem: {fname} =====")
    Xf = feats[feature_cols].fillna(0).values
    Xf_scaled = scaler.transform(Xf)
    yf = feats['label'].values
    n_points = len(feats)

    for mname, model in trained_models.items():
        yp = model.predict(Xf_scaled)
        acc = accuracy_score(yf, yp)
        crep = classification_report(yf, yp, zero_division=0)
        cm = confusion_matrix(yf, yp)
        print(f"\n-> Modelo: {mname}  |  Acur√°cia: {acc:.4f}")
        print(crep)
        print("Matriz de confus√£o:")
        print(cm)
        summary.append({'file': fname, 'model': mname, 'accuracy': acc, 'confusion': cm})

        # ----- Plot 3D de TP/TN/FP/FN -----
        # prepara df de visualiza√ß√£o
        vis = feats[['x','y','z','label']].copy()
        vis['pred'] = yp
        tp = vis[(vis['label']==1) & (vis['pred']==1)]
        tn = vis[(vis['label']==0) & (vis['pred']==0)]
        fp = vis[(vis['label']==0) & (vis['pred']==1)]
        fn = vis[(vis['label']==1) & (vis['pred']==0)]

        fig = plt.figure(figsize=(8,6))
        ax = fig.add_subplot(111, projection='3d')
        if len(tn): ax.scatter(tn['x'], tn['y'], tn['z'], c='green', s=3, label='Normal ‚úÖ')
        if len(tp): ax.scatter(tp['x'], tp['y'], tp['z'], c='red', s=6, label='Outlier ‚úÖ')
        if len(fp): ax.scatter(fp['x'], fp['y'], fp['z'], c='orange', s=8, label='Falso Positivo')
        if len(fn): ax.scatter(fn['x'], fn['y'], fn['z'], c='blue', s=8, label='Falso Negativo')
        ax.set_title(f"{os.path.splitext(fname)[0]} - {mname} (acc={acc:.3f})")
        ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')
        ax.legend(loc='upper right')
        plt.tight_layout()

        if SAVE_PLOTS:
            outfig = os.path.join(OUT_FOLDER, f"{os.path.splitext(fname)[0]}_{mname}.png")
            plt.savefig(outfig, dpi=200)
            print(f"Plot salvo: {outfig}")
        if PLOT_SHOW:
            plt.show()
        plt.close(fig)

# -----------------------------
# Resumo final
# -----------------------------
df_summary = pd.DataFrame(summary)
print("\n\n=== Resumo final por arquivo e modelo ===")
print(df_summary.groupby(['file','model'])['accuracy'].mean().unstack(fill_value=0))
print(f"\nFiguras e resultados salvos em: {OUT_FOLDER}")

import joblib

joblib.dump(trained_models['RandomForest'], "random_forest_model.pkl")
joblib.dump(trained_models['ExtraTrees'], "extra_trees_model.pkl")
joblib.dump(trained_models['GradientBoosting'], "gradient_boosting_model.pkl")
joblib.dump(trained_models['MLP'], "mlp_model.pkl")

print("Modelos salvos!")

import joblib

rf = joblib.load("random_forest_model.pkl")
et = joblib.load("extra_trees_model.pkl") # Assuming 'svm' was meant to be ExtraTrees
gb = joblib.load("gradient_boosting_model.pkl") # Assuming 'knn' was meant to be GradientBoosting
mlp = joblib.load("mlp_model.pkl")

print("Modelos carregados!")

# ================================
# IMPORTAR NUVEM REAL EM UMA √öNICA C√âLULA
# ================================

import pandas as pd
from google.colab import files
import os

print("üìÇ Fa√ßa upload do arquivo da nuvem real (.txt)")
uploaded = files.upload()

# Encontrar automaticamente o arquivo .txt enviado
arquivo_txt = None
for arq in uploaded.keys():
    if arq.lower().endswith(".txt"):
        arquivo_txt = arq
        break

if arquivo_txt is None:
    raise ValueError("Nenhum arquivo .txt foi enviado!")

print(f"‚úî Arquivo detectado: {arquivo_txt}")

# Verificar se possui cabe√ßalho
with open(arquivo_txt, 'r') as f:
    primeira_linha = f.readline().strip()

tem_header = any(col in primeira_linha.lower() for col in ["x", "y", "z", "label"])

if tem_header:
    print("‚úî Cabe√ßalho detectado ‚Äî lendo normalmente")
    df_real = pd.read_csv(arquivo_txt, sep=r"\s+")
else:
    print("‚ö† Sem cabe√ßalho ‚Äî adicionando nomes x y z label")
    df_real = pd.read_csv(arquivo_txt, sep=r"\s+", header=None, names=["x","y","z","label"])

print("\nPr√©via da nuvem carregada:")
display(df_real.head())

# Criar X_real e y_real
X_real = df_real[["x","y","z"]].values
y_real = df_real["label"].values

print("\n‚úî Dados prontos para o modelo!")
print("Formato X_real:", X_real.shape)
print("Formato y_real:", y_real.shape)

# ===========================================
# RODAR OS 4 MODELOS NA NUVEM REAL + VALIDAR
# ===========================================

import joblib
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# --- Feature extraction and scaling for X_real (to match training data) ---
print("üîç Extraindo features da nuvem real para predi√ß√£o...")
df_real_feats = extract_features_for_cloud(df_real)

# Select the feature columns and scale them using the pre-fitted scaler
X_real_processed = scaler.transform(df_real_feats[feature_cols].fillna(0).values)

# Keep y_real as is, as it's already the ground truth labels

# Lista de modelos para carregar e testar (atualizado para os modelos realmente salvos)
modelos = {
    "RandomForest": "random_forest_model.pkl",
    "ExtraTrees": "extra_trees_model.pkl",
    "GradientBoosting": "gradient_boosting_model.pkl",
    "MLP": "mlp_model.pkl"
}

def ajustar_predicoes(modelo_nome, y_pred):
    """
    Ajusta as predi√ß√µes para o padr√£o:
    1 = OUTLIER
    0 = NORMAL
    """

    # Para estes modelos, 0 e 1 j√° s√£o os valores esperados
    return y_pred


# Rodar cada modelo
for nome, arquivo in modelos.items():

    print("\n=============================================")
    print(f"‚≠ê MODELO: {nome}")
    print("=============================================")

    # Carregar o modelo
    model = joblib.load(arquivo)

    # Gerar predi√ß√µes usando os dados X_real_processed
    y_pred = model.predict(X_real_processed)

    # Ajustar r√≥tulos para 0/1 (para os modelos listados, n√£o √© necess√°rio ajuste)
    y_pred_adj = ajustar_predicoes(nome, y_pred)

    # Matriz de confus√£o
    print("\nüìå Matriz de confus√£o:")
    print(confusion_matrix(y_real, y_pred_adj))

    # Relat√≥rio
    print("\nüìå Relat√≥rio de classifica√ß√£o:")
    print(classification_report(y_real, y_pred_adj, digits=3, zero_division=0))

"""# Avalia√ß√£o dos modelos:
# ====
# RANDON FOREST
# Identificou 98.4% dos pontos normais (classe 0).
# Identificou 67% dos outliers (classe 1).
# F1-score geral: 0.96 ‚Üí excelente!
# =====
# EXTRATREES
# O modelo praticamente confundiu a classe 0, achando outlier em tudo.
# A recall da classe 1 √© muito alta (0.95), mas classifica tudo como outilier
# A precis√£o da classe 1 √© p√©ssima (0.106)
# =====
# GRADIENTE BOOSTING
# Muito parecido com o RandomForest
# Ligeiramente pior na recall da classe 1 (0.63)
# Excelente para classe 0 (98.4%).
# =====
# MPL (rede neural)
# Excelente para classe 0 (98.4%).
# Recall da classe 1 = 0.56, indentificou captou poucos outliers
# F1 score da classe 1 = 0.39 (ruim)
# =====
# Conclus√£o: O melhor desempenho foi o RF muito proximo do GXboost, os outros dois foram muito ruins.

"""

import joblib

# Assuming X_real, y_real are available from previous cells (pdJqIQx7HSxG)
# X_real is the raw x, y, z coordinates
# y_real is the ground truth labels

# Lista de modelos (redefinida aqui para garantir acesso)
modelos = {
    "RandomForest": "random_forest_model.pkl",
    "ExtraTrees": "extra_trees_model.pkl",
    "GradientBoosting": "gradient_boosting_model.pkl",
    "MLP": "mlp_model.pkl"
}

# Loop para gerar e plotar resultados para cada modelo
for nome, arquivo in modelos.items():
    print(f"\nGerando plot 3D para o modelo: {nome}...")

    # Carregar o modelo
    model = joblib.load(arquivo)

    # Gerar predi√ß√µes usando os dados X_real_processed (que foi usado para a avalia√ß√£o)
    # X_real_processed e y_real s√£o do passo de avalia√ß√£o anterior (Q_sY1iQaH-4u)
    y_pred = model.predict(X_real_processed)

    # Como ajustamos a fun√ß√£o 'ajustar_predicoes' para retornar y_pred diretamente para esses modelos,
    # y_pred_adj √© simplesmente y_pred.
    y_pred_adj = y_pred

    # Chamar a fun√ß√£o de plotagem com os dados originais (X_real) e as predi√ß√µes
    plot_3d_results(nome, X_real, y_real, y_pred_adj)

print("Plots 3D gerados para todos os modelos!")